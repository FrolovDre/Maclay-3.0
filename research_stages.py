"""
Research stages and prompts for AI Research Assistant
"""

import asyncio
import httpx
from typing import Dict, List, Any
import os
import json
import re
import pdfplumber
import json
from datetime import datetime

class ResearchStage:
    """Base class for research stages"""
    
    def __init__(self, name: str, description: str, icon: str):
        self.name = name
        self.description = description
        self.icon = icon
        self.status = "pending"  # pending, active, completed, error
        self.progress = 0
        self.result = None
        self.error = None

class ResearchProcessor:
    """Main processor for research stages"""
    
    def __init__(self, config, manager, client_id: str):
        self.config = config
        self.manager = manager
        self.client_id = client_id
        self.stages = []
        self.current_stage = 0

    async def _call_deepseek(self, prompt: str, temperature: float = 0.7, max_new_tokens: int = 4096) -> str:
        """Call DeepSeek model via Hugging Face Inference API"""
        api_url = f"{self.config.HF_API_URL}/models/{self.config.HF_MODEL}"
        headers = {
            "Authorization": f"Bearer {self.config.HF_API_TOKEN}",
            "Content-Type": "application/json",
        }
        payload = {
            "inputs": prompt,
            "parameters": {
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "return_full_text": False,
            },
        }

        async with httpx.AsyncClient(timeout=270.0) as client:
            max_retries = 5
            attempt = 0
            while attempt < max_retries:
                try:
                    response = await client.post(api_url, headers=headers, json=payload)

                    if response.status_code == 503:
                        await asyncio.sleep(5)
                        continue

                    response.raise_for_status()
                    result = response.json()
                    return self._extract_generated_text(result)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_retries:
                        raise e
                    await asyncio.sleep(2 ** attempt)

    def _extract_generated_text(self, result: Any) -> str:
        """Extract generated text from Hugging Face Inference response"""
        if isinstance(result, list) and result:
            item = result[0]
            if isinstance(item, dict):
                return item.get("generated_text") or item.get("text") or ""
            if isinstance(item, str):
                return item
        if isinstance(result, dict):
            return result.get("generated_text") or result.get("text") or ""
        return ""
        
    async def send_update(self, stage_name: str, status: str, progress: int, message: str = ""):
        """Send update to client via WebSocket"""
        await self.manager.send_message(self.client_id, {
            "type": "stage_update",
            "stage": stage_name,
            "status": status,
            "progress": progress,
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    async def _execute_with_retry(self, func, *args, stage_name: str, stage_description: str, max_retries: int = 3):
        """Execute function with retry mechanism"""
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –¥–ª—è {stage_description}")
                
                if attempt > 0:
                    await self.send_update(stage_name, "active", 0, f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}...")
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff: 2, 4, 8 seconds
                
                result = await func(*args)
@@ -188,117 +236,68 @@ class ResearchProcessor:
            return {
                "success": False,
                "error": str(e),
                "error_details": error_details
            }
    
    async def collect_market_data(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1: Collect market data with retry mechanism"""
        return await self._execute_with_retry(
            self._collect_market_data_internal,
            research_data,
            research_type,
            stage_name="data_collection",
            stage_description="—Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"
        )
    
    async def _collect_market_data_internal(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Internal method for data collection"""
        await self.send_update("data_collection", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å...")
        
        prompt = self.get_data_collection_prompt(research_data, research_type)
        print(f"üìù –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        await self.send_update("data_collection", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
        
        try:
            await self.send_update("data_collection", "active", 40, "–í—ã–ø–æ–ª–Ω—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å...")
            content = await self._call_deepseek(prompt, temperature=0.7, max_new_tokens=2048)
            await self.send_update("data_collection", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")

            print(f"‚úÖ –û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            await self.send_update("data_collection", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")

            market_data = self.parse_market_data(content, research_type)

            await self.send_update("data_collection", "completed", 100, f"–ù–∞–π–¥–µ–Ω–æ {len(market_data.get('companies', []))} –∫–æ–º–ø–∞–Ω–∏–π")

            return market_data
        except Exception as e:
            error_msg = f"API Error: {str(e)}"
            print(f"‚ùå {error_msg}")
            await self.send_update("data_collection", "error", 0, error_msg)
            raise

    async def collect_local_documents_insights(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1.5: Extract and summarize insights from local PDFs with retry"""
        return await self._execute_with_retry(
            self._collect_local_documents_insights_internal,
            research_data,
            research_type,
            stage_name="local_documents",
            stage_description="–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF"
        )

    def _read_pdf_text(self, file_path: str, max_chars: int = None) -> str:
        """Extract text from a PDF file - full text extraction"""
        text_parts: List[str] = []
        try:
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text() or ""
                    if page_text:
                        text_parts.append(page_text)
                    # Remove character limit - extract full text
                    # if max_chars and sum(len(p) for p in text_parts) >= max_chars:
                    #     break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
@@ -331,297 +330,163 @@ class ResearchProcessor:
        # Process each PDF file with progress updates
        for i, f in enumerate(pdf_files):
            progress = int((i / len(pdf_files)) * 40) + 10  # 10-50%
            await self.send_update("local_documents", "active", progress, 
                                 f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {i+1}/{len(pdf_files)}")
            
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF {i+1}/{len(pdf_files)}: {os.path.basename(f)}")
            
            text = self._read_pdf_text(f)  # Extract full text without character limit
            total_chars += len(text)
            files_payload.append({
                "file": os.path.basename(f),
                "excerpt": text
            })
            
            print(f"üìä PDF {i+1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Small delay to show progress
            await asyncio.sleep(0.2)
        
        await self.send_update("local_documents", "active", 55, f"–¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        prompt = self.get_local_documents_prompt(files_payload, research_data, research_type)
        await self.send_update("local_documents", "active", 65, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        try:
            await self.send_update("local_documents", "active", 70, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
            content = await self._call_deepseek(prompt, temperature=0.2, max_new_tokens=1024)
            await self.send_update("local_documents", "active", 85, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –ò–ò...")
        except Exception as e:
            await self.send_update("local_documents", "error", 0, f"API Error: {e}")
            return {"insights": [], "files": [f["file"] for f in files_payload]}
        
        await self.send_update("local_documents", "active", 90, "–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã...")
        insights = self.parse_local_insights(content)
        
        # Count insights by source file
        insights_by_file = {}
        for insight in insights:
            source_file = insight.get("source_file", "unknown.pdf")
            if source_file not in insights_by_file:
                insights_by_file[source_file] = 0
            insights_by_file[source_file] += 1
        
        # Create summary message without specific file names
        summary = f"–ù–∞–π–¥–µ–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        
        await self.send_update("local_documents", "completed", 100, summary)
        
        print(f"üìà –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò PDF:")
        print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files_payload)}")
        print(f"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–Ω—Å–∞–π—Ç–æ–≤: {len(insights)}")
        for file, count in insights_by_file.items():
            print(f"   {file}: {count} –∏–Ω—Å–∞–π—Ç–æ–≤")
        
        return {"insights": insights, "files": [f["file"] for f in files_payload]}
    
    async def analyze_cases(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Stage 2: Analyze cases with retry mechanism"""
        return await self._execute_with_retry(
            self._analyze_cases_internal,
            market_data,
            research_data,
            research_type,
            stage_name="case_analysis",
            stage_description="–∞–Ω–∞–ª–∏–∑–∞ –∫–µ–π—Å–æ–≤"
        )
    
    async def _analyze_cases_internal(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Internal method for case analysis"""
        await self.send_update("case_analysis", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–µ–π—Å–æ–≤...")
        
        prompt = self.get_case_analysis_prompt(market_data, research_data, research_type)
        
        await self.send_update("case_analysis", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑...")
        
        try:
            content = await self._call_deepseek(prompt, temperature=0.5, max_new_tokens=2048)
            await self.send_update("case_analysis", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞...")

            await self.send_update("case_analysis", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –∫–µ–π—Å—ã...")

            cases = self.parse_cases(content)
            await self.send_update("case_analysis", "completed", 100, f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cases)} –∫–µ–π—Å–æ–≤")

            return cases
        except Exception as e:
            raise Exception(f"API Error: {e}")
    
    
    
    
    async def generate_report(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Stage 4: Generate final report with retry mechanism"""
        return await self._execute_with_retry(
            self._generate_report_internal,
            cases,
            research_data,
            research_type,
            stage_name="report_generation",
            stage_description="–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
        )
    
    async def _generate_report_internal(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Internal method for report generation"""
        await self.send_update("report_generation", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞...")
        
        prompt = self.get_report_generation_prompt(cases, research_data, research_type)
        
        await self.send_update("report_generation", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–∞...")
        
        try:
            response_text = await self._call_deepseek(prompt, temperature=0.3, max_new_tokens=4096)

            await self.send_update("report_generation", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")
            await self.send_update("report_generation", "active", 90, "–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç...")

            report_content = self.extract_report_content(response_text)

            # Enhance report with additional links
            await self.send_update("report_generation", "active", 95, "–î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏...")
            enhanced_report = await self.enhance_report_with_links(report_content, cases, research_data, research_type)

            # Clean the report content before final processing
            final_report = self.clean_report_content(enhanced_report)

            # Final report length check
            print(f"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
            print(f"   –î–ª–∏–Ω–∞ –æ—Ç—á–µ—Ç–∞: {len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤: {final_report.count(chr(10)) + 1}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫: {final_report.count('[')}")

            await self.send_update("report_generation", "completed", 100, f"–û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤! ({len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤)")

            return final_report
        except Exception as e:
            raise Exception(f"API Error: {e}")
    
    def get_data_collection_prompt(self, research_data: Dict[str, Any], research_type: str) -> str:
        """Get prompt for data collection stage"""
        if research_type == "feature":
            return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∏ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –æ —Ñ–∏–Ω—Ç–µ—Ö-–ø—Ä–æ–¥—É–∫—Ç–∞—Ö.

–¶–ï–õ–¨: –ù–∞–π—Ç–∏ –∏ —Å–æ–±—Ä–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–û–î–†–û–ë–ù–£–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ñ–∏—á—É "{research_data.get('research_element', '')}".

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –≠–ª–µ–º–µ–Ω—Ç: {research_data.get('research_element', '')}
- –ë–µ–Ω—á–º–∞—Ä–∫–∏: {research_data.get('benchmarks', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–≥—Ä–æ–∫–∏: {research_data.get('required_players', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã: {research_data.get('required_countries', '')}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ü–û–ò–°–ö –°–°–´–õ–û–ö:
1. –ù–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 15-20 –∫–æ–º–ø–∞–Ω–∏–π
2. –î–ª—è –ö–ê–ñ–î–û–ô –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 8-10 –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –°–°–´–õ–û–ö:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –∫–æ–º–ø–∞–Ω–∏–∏
   - –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ (LinkedIn, Twitter, Facebook)
   - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–∑—ã–≤—ã
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
@@ -1071,135 +936,115 @@ class ResearchProcessor:
                        "fact": item.get("fact") or "",
                        "metrics": item.get("metrics") or None,
                        "date": item.get("date") or None,
                        "links": item.get("links") or []
                    })
                return norm
        except Exception:
            pass
        # Fallback: extract lines starting with '-' or '*'
        insights: List[Dict[str, Any]] = []
        for line in content_str.split('\n'):
            line = line.strip(" -‚Ä¢*")
            if not line:
                continue
            insights.append({
                "source_file": "unknown.pdf",
                "download_link": None,  # Don't create link for unknown files
                "section": "",
                "fact": line,
                "metrics": None,
                "date": None,
                "links": []
            })
        return insights
    
    def parse_market_data(self, content: str, research_type: str) -> Dict[str, Any]:
        """Parse market data from generated text"""
        try:
            companies = self.extract_companies_from_text(content)

            return {
                "raw_content": content,
                "companies": companies,
                "research_type": research_type,
                "timestamp": datetime.now().isoformat(),
                "total_found": len(companies)
            }
        except Exception as e:
            return {
                "raw_content": f"Error parsing data: {str(e)}",
                "companies": [],
                "research_type": research_type,
                "timestamp": datetime.now().isoformat(),
                "total_found": 0
            }
    
    def extract_companies_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract company/product information from text"""
        companies = []
        lines = text.split('\n')
        
        current_company = {}
        for line in lines:
            line = line.strip()
            if not line:
                if current_company:
                    companies.append(current_company)
                    current_company = {}
                continue
                
            # Look for company/product patterns - support both
            if any(keyword in line.lower() for keyword in ['–∫–æ–º–ø–∞–Ω–∏—è:', 'company:', '–Ω–∞–∑–≤–∞–Ω–∏–µ:', 'name:', '–ø—Ä–æ–¥—É–∫—Ç:', 'product:']):
                if current_company:
                    companies.append(current_company)
                current_company = {"name": line.split(':', 1)[1].strip() if ':' in line else line}
            elif any(keyword in line.lower() for keyword in ['—Å–∞–π—Ç:', 'website:', 'url:']):
                if current_company:
                    current_company["website"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif any(keyword in line.lower() for keyword in ['—Å—Ç—Ä–∞–Ω–∞:', 'country:']):
                if current_company:
                    current_company["country"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif any(keyword in line.lower() for keyword in ['—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:', 'characteristics:']):
                if current_company:
                    current_company["characteristics"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif line.startswith('http'):
                if current_company:
                    if "links" not in current_company:
                        current_company["links"] = []
                    current_company["links"].append(line)
        
        if current_company:
            companies.append(current_company)
            
        return companies
    
    def parse_cases(self, content: str) -> List[Dict[str, Any]]:
        """Parse cases from generated text"""
        try:
            return self.extract_cases_from_text(content)
        except Exception:
            return []
    
    def extract_cases_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract case information from text"""
        cases = []
        lines = text.split('\n')
        
        current_case = {}
        case_number = 1
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for case patterns - support both feature and product cases
            if (line.startswith(f'**–ö–µ–π—Å {case_number}') or 
                line.startswith(f'–ö–µ–π—Å {case_number}') or
                line.startswith(f'**–ü—Ä–æ–¥—É–∫—Ç {case_number}') or
                line.startswith(f'–ü—Ä–æ–¥—É–∫—Ç {case_number}')):
                if current_case:
                    cases.append(current_case)
                current_case = {
                    "number": case_number,
                    "title": line.replace('**', '').replace('*', '').strip()
@@ -1240,167 +1085,115 @@ class ResearchProcessor:
        broken_links = 0
        
        for case in cases:
            if "verified_links" in case:
                total_links += len(case["verified_links"])
                working_links += len([link for link in case["verified_links"] if link.get("status") == "working"])
            if "broken_links" in case:
                broken_links += len(case["broken_links"])
        
        percentage = (working_links/total_links*100) if total_links > 0 else 0
        
        verification_summary = f"""

## –°–≤–æ–¥–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫

- **–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ:** {total_links}
- **–†–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {working_links}
- **–ù–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {broken_links}
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {percentage:.1f}%

*–í—Å–µ —Å—Å—ã–ª–∫–∏ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.*
"""
        
        return report_content + verification_summary
    
    def extract_report_content(self, response_text: str) -> str:
        """Extract report content from generated text"""
        return response_text or "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
    
    async def enhance_report_with_links(self, report_content: str, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Enhance report with additional links from verified sources"""
        try:
            # Extract all verified links from cases
            all_verified_links = []
            for case in cases:
                if "verified_links" in case:
                    for link in case["verified_links"]:
                        if link.get("status") == "working":
                            all_verified_links.append({
                                "url": link.get("url"),
                                "company": case.get("title", case.get("company", "Unknown")),
                                "context": case.get("description", "")
                            })
            
            if not all_verified_links:
                return report_content
            
            # Create prompt for link enhancement
            # Use full report content, but limit to reasonable size for API
            max_content_length = 15000  # Increased from 3000
            report_preview = report_content[:max_content_length]
            if len(report_content) > max_content_length:
                report_preview += "\n\n[... –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –æ—Ç—á–µ—Ç–∞ ...]"
            
            prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç—ã. –£–ª—É—á—à–∏ –æ—Ç—á–µ—Ç, –¥–æ–±–∞–≤–∏–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–û–¢–ß–ï–¢ –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:
{report_preview}

–ü–†–û–í–ï–†–ï–ù–ù–´–ï –°–°–´–õ–ö–ò:
{json.dumps(all_verified_links[:20], ensure_ascii=False, indent=2)}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ –≤ –æ—Ç—á–µ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤
2. –î–æ–±–∞–≤—å –∫ –Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: [—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)
4. –ù–ï –∏–∑–º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á–µ—Ç–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏
5. –ú–∞–∫—Å–∏–º—É–º 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü
6. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã > –∫–µ–π—Å—ã > –Ω–æ–≤–æ—Å—Ç–∏
7. –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –ü–û–õ–ù–´–ô –æ—Ç—á–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –Ω–µ –æ–±—Ä–µ–∑–∞–π –µ–≥–æ

–í–ï–†–ù–ò –ü–û–õ–ù–´–ô –£–õ–£–ß–®–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –° –î–û–ë–ê–í–õ–ï–ù–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò.
"""
            
            try:
                enhanced_content = await self._call_deepseek(prompt, temperature=0.3, max_new_tokens=4096)

                if enhanced_content:
                    print(f"üìä –£–õ–£–ß–®–ï–ù–ò–ï –û–¢–ß–ï–¢–ê:")
                    print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {len(report_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"   –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {len(enhanced_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return enhanced_content
                else:
                    print(f"‚ö†Ô∏è –ò–ò –Ω–µ –≤–µ—Ä–Ω—É–ª —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π")
                    return report_content
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
                return report_content

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
            return report_content
    
    async def verify_report_links(self, report_content: str) -> str:
        """Verify all links in the report and remove broken ones"""
        try:
            import re
            
            # Find all markdown links in the report
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            links = re.findall(link_pattern, report_content)
            
            if not links:
                print("üìã –°—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return report_content
            
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            verified_links = []
            broken_links = []
            
            # Check each link
            for i, (text, url) in enumerate(links):
                print(f"üîó –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫—É {i+1}/{len(links)}: {url}")
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {percentage:.1f}%

*–í—Å–µ —Å—Å—ã–ª–∫–∏ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.*
"""
        
        return report_content + verification_summary
    
    def extract_report_content(self, api_response: Dict[str, Any]) -> str:
        """Extract report content from API response"""
        try:
            if "candidates" in api_response and len(api_response["candidates"]) > 0:
                content = api_response["candidates"][0]["content"]["parts"][0]["text"]
                return content
            else:
                return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
    
    async def enhance_report_with_links(self, report_content: str, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Enhance report with additional links from verified sources"""
        try:
            # Extract all verified links from cases
            all_verified_links = []
            for case in cases:
                if "verified_links" in case:
                    for link in case["verified_links"]:
                        if link.get("status") == "working":
                            all_verified_links.append({
                                "url": link.get("url"),
                                "company": case.get("title", case.get("company", "Unknown")),
                                "context": case.get("description", "")
                            })
            
            if not all_verified_links:
                return report_content
            
            # Create prompt for link enhancement
            # Use full report content, but limit to reasonable size for API
            max_content_length = 15000  # Increased from 3000
            report_preview = report_content[:max_content_length]
            if len(report_content) > max_content_length:
                report_preview += "\n\n[... –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –æ—Ç—á–µ—Ç–∞ ...]"
            
            prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç—ã. –£–ª—É—á—à–∏ –æ—Ç—á–µ—Ç, –¥–æ–±–∞–≤–∏–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–û–¢–ß–ï–¢ –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:
{report_preview}

–ü–†–û–í–ï–†–ï–ù–ù–´–ï –°–°–´–õ–ö–ò:
{json.dumps(all_verified_links[:20], ensure_ascii=False, indent=2)}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ –≤ –æ—Ç—á–µ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤
2. –î–æ–±–∞–≤—å –∫ –Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: [—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)
4. –ù–ï –∏–∑–º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á–µ—Ç–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏
5. –ú–∞–∫—Å–∏–º—É–º 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü
6. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã > –∫–µ–π—Å—ã > –Ω–æ–≤–æ—Å—Ç–∏
7. –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –ü–û–õ–ù–´–ô –æ—Ç—á–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –Ω–µ –æ–±—Ä–µ–∑–∞–π –µ–≥–æ

–í–ï–†–ù–ò –ü–û–õ–ù–´–ô –£–õ–£–ß–®–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –° –î–û–ë–ê–í–õ–ï–ù–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò.
"""
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                # Retry logic for 503 errors (model overloaded)
                max_retries = 5
                attempt = 0
                
                while attempt < max_retries:
                    try:
                        response = await client.post(
                            f"{self.config.GEMINI_API_URL}/v1beta/models/{self.config.GEMINI_MODEL}:generateContent",
                            headers={
                                "Content-Type": "application/json",
                                "x-goog-api-key": self.config.GEMINI_API_KEY
                            },
                            json={
                                "contents": [{
                                    "parts": [{"text": prompt}]
                                }],
                                "generationConfig": {
                                    "temperature": 0.3
                                }
                            }
                        )
                        
                        # Check for 503 error (model overloaded)
                        if response.status_code == 503:
                            error_data = response.json() if response.headers.get('content-type', '').startswith('application/json') else {}
                            error_message = error_data.get('error', {}).get('message', 'Model overloaded')
                            
                            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–∞ (503), –ø–æ–≤—Ç–æ—Ä—è–µ–º —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥... (–ø–æ–ø—ã—Ç–∫–∞ –Ω–µ –∑–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è)")
                            await asyncio.sleep(5)  # Wait 5 seconds before retry
                            # –ù–ï —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º attempt –¥–ª—è 503 –æ—à–∏–±–∫–∏ - –Ω–µ —Ç—Ä–∞—Ç–∏–º –ø–æ–ø—ã—Ç–∫–∏
                            continue
                        
                        # If not 503, break out of retry loop
                        break
                        
                    except Exception as e:
                        attempt += 1
                        print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}: {str(e)}")
                        if attempt < max_retries:
                            await asyncio.sleep(2 ** attempt)  # Exponential backoff
                        else:
                            raise e
                
                if response.status_code == 200:
                    result = response.json()
                    enhanced_content = result.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")
                    
                    if enhanced_content:
                        print(f"üìä –£–õ–£–ß–®–ï–ù–ò–ï –û–¢–ß–ï–¢–ê:")
                        print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {len(report_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        print(f"   –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {len(enhanced_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                        return enhanced_content
                    else:
                        print(f"‚ö†Ô∏è –ò–ò –Ω–µ –≤–µ—Ä–Ω—É–ª —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π")
                        return report_content
                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {response.status_code}")
                    return report_content
                    
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
            return report_content
    
    async def verify_report_links(self, report_content: str) -> str:
        """Verify all links in the report and remove broken ones"""
        try:
            import re
            
            # Find all markdown links in the report
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            links = re.findall(link_pattern, report_content)
            
            if not links:
                print("üìã –°—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return report_content
            
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            verified_links = []
            broken_links = []
            
            # Check each link
            for i, (text, url) in enumerate(links):
                print(f"üîó –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫—É {i+1}/{len(links)}: {url}")
                
                try:
                    # Skip PDF links to our domain - they should work
                    if url.startswith(f'{self.config.BASE_URL}/data/'):
                        verified_links.append((text, url))
                        print(f"‚úÖ PDF —Å—Å—ã–ª–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞: {url}")
                        continue
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        response = await client.head(url, follow_redirects=True)
                        if response.status_code < 400:
                            verified_links.append((text, url))
                            print(f"‚úÖ –°—Å—ã–ª–∫–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç: {response.status_code}")
                        else:
                            broken_links.append((text, url))
                            print(f"‚ùå –°—Å—ã–ª–∫–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {response.status_code}, —É–¥–∞–ª—è–µ–º")
                            
                except Exception as e:
                    broken_links.append((text, url))
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–∫–∏: {str(e)}, —É–¥–∞–ª—è–µ–º")
            
            # Remove broken links and their text from report
            if broken_links:
                print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º {len(broken_links)} –Ω–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫ —Å —Ç–µ–∫—Å—Ç–æ–º")
                for text, url in broken_links:
                    # Remove the entire link with text completely
                    report_content = report_content.replace(f"[{text}]({url})", "")
                
                # Clean up extra whitespace and empty lines
                import re
                report_content = re.sub(r'\n\s*\n\s*\n', '\n\n', report_content)  # Remove multiple empty lines
                report_content = re.sub(r'^\s*\n', '', report_content, flags=re.MULTILINE)  # Remove empty lines at start
                report_content = report_content.strip()
            
            # Replace original links with verified alternatives
            for text, url in verified_links:
                # Find and replace the original link with the verified one
                original_pattern = f"[{text}]("
                if original_pattern in report_content:
                    # Find the original link and replace it
                    import re
                    pattern = f"\\[{re.escape(text)}\\]\\([^)]+\\)"
                    replacement = f"[{text}]({url})"
                    report_content = re.sub(pattern, replacement, report_content)
            
            print(f"üìä –ò–¢–û–ì–ò –ü–†–û–í–ï–†–ö–ò –°–°–´–õ–û–ö –í –û–¢–ß–ï–¢–ï:")
            print(f"   –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {len(links)}")
            print(f"   –†–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫: {len(verified_links)}")
            print(f"   –ù–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫: {len(broken_links)}")
            if len(links) > 0:
                percentage = (len(verified_links) / len(links)) * 100
                print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö: {percentage:.1f}%")
            
            return report_contenimport pdfplumber
import json
from datetime import datetime

class ResearchStage:
    """Base class for research stages"""
    
    def __init__(self, name: str, description: str, icon: str):
        self.name = name
        self.description = description
        self.icon = icon
        self.status = "pending"  # pending, active, completed, error
        self.progress = 0
        self.result = None
        self.error = None

class ResearchProcessor:
    """Main processor for research stages"""
    
    def __init__(self, config, manager, client_id: str):
        self.config = config
        self.manager = manager
        self.client_id = client_id
        self.stages = []
        self.current_stage = 0

    async def _call_deepseek(self, prompt: str, temperature: float = 0.7, max_new_tokens: int = 4096) -> str:
        """Call DeepSeek model via Hugging Face Inference API"""
        api_url = f"{self.config.HF_API_URL}/models/{self.config.HF_MODEL}"
        headers = {
            "Authorization": f"Bearer {self.config.HF_API_TOKEN}",
            "Content-Type": "application/json",
        }
        payload = {
            "inputs": prompt,
            "parameters": {
                "temperature": temperature,
                "max_new_tokens": max_new_tokens,
                "return_full_text": False,
            },
        }

        async with httpx.AsyncClient(timeout=270.0) as client:
            max_retries = 5
            attempt = 0
            while attempt < max_retries:
                try:
                    response = await client.post(api_url, headers=headers, json=payload)

                    if response.status_code == 503:
                        await asyncio.sleep(5)
                        continue

                    response.raise_for_status()
                    result = response.json()
                    return self._extract_generated_text(result)
                except Exception as e:
                    attempt += 1
                    if attempt >= max_retries:
                        raise e
                    await asyncio.sleep(2 ** attempt)

    def _extract_generated_text(self, result: Any) -> str:
        """Extract generated text from Hugging Face Inference response"""
        if isinstance(result, list) and result:
            item = result[0]
            if isinstance(item, dict):
                return item.get("generated_text") or item.get("text") or ""
            if isinstance(item, str):
                return item
        if isinstance(result, dict):
            return result.get("generated_text") or result.get("text") or ""
        return ""
        
    async def send_update(self, stage_name: str, status: str, progress: int, message: str = ""):
        """Send update to client via WebSocket"""
        await self.manager.send_message(self.client_id, {
            "type": "stage_update",
            "stage": stage_name,
            "status": status,
            "progress": progress,
            "message": message,
            "timestamp": datetime.now().isoformat()
        })
    
    async def _execute_with_retry(self, func, *args, stage_name: str, stage_description: str, max_retries: int = 3):
        """Execute function with retry mechanism"""
        last_exception = None
        
        for attempt in range(max_retries):
            try:
                print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –¥–ª—è {stage_description}")
                
                if attempt > 0:
                    await self.send_update(stage_name, "active", 0, f"–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}...")
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff: 2, 4, 8 seconds
                
                result = await func(*args)
@@ -188,117 +236,68 @@ class ResearchProcessor:
            return {
                "success": False,
                "error": str(e),
                "error_details": error_details
            }
    
    async def collect_market_data(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1: Collect market data with retry mechanism"""
        return await self._execute_with_retry(
            self._collect_market_data_internal,
            research_data,
            research_type,
            stage_name="data_collection",
            stage_description="—Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"
        )
    
    async def _collect_market_data_internal(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Internal method for data collection"""
        await self.send_update("data_collection", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å...")
        
        prompt = self.get_data_collection_prompt(research_data, research_type)
        print(f"üìù –ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        await self.send_update("data_collection", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
        
        try:
            await self.send_update("data_collection", "active", 40, "–í—ã–ø–æ–ª–Ω—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å...")
            content = await self._call_deepseek(prompt, temperature=0.7, max_new_tokens=2048)
            await self.send_update("data_collection", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")

            print(f"‚úÖ –û—Ç–≤–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
            await self.send_update("data_collection", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ...")

            market_data = self.parse_market_data(content, research_type)

            await self.send_update("data_collection", "completed", 100, f"–ù–∞–π–¥–µ–Ω–æ {len(market_data.get('companies', []))} –∫–æ–º–ø–∞–Ω–∏–π")

            return market_data
        except Exception as e:
            error_msg = f"API Error: {str(e)}"
            print(f"‚ùå {error_msg}")
            await self.send_update("data_collection", "error", 0, error_msg)
            raise

    async def collect_local_documents_insights(self, research_data: Dict[str, Any], research_type: str) -> Dict[str, Any]:
        """Stage 1.5: Extract and summarize insights from local PDFs with retry"""
        return await self._execute_with_retry(
            self._collect_local_documents_insights_internal,
            research_data,
            research_type,
            stage_name="local_documents",
            stage_description="–æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö PDF"
        )

    def _read_pdf_text(self, file_path: str, max_chars: int = None) -> str:
        """Extract text from a PDF file - full text extraction"""
        text_parts: List[str] = []
        try:
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text() or ""
                    if page_text:
                        text_parts.append(page_text)
                    # Remove character limit - extract full text
                    # if max_chars and sum(len(p) for p in text_parts) >= max_chars:
                    #     break
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF {file_path}: {e}")
@@ -331,297 +330,163 @@ class ResearchProcessor:
        # Process each PDF file with progress updates
        for i, f in enumerate(pdf_files):
            progress = int((i / len(pdf_files)) * 40) + 10  # 10-50%
            await self.send_update("local_documents", "active", progress, 
                                 f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç {i+1}/{len(pdf_files)}")
            
            print(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF {i+1}/{len(pdf_files)}: {os.path.basename(f)}")
            
            text = self._read_pdf_text(f)  # Extract full text without character limit
            total_chars += len(text)
            files_payload.append({
                "file": os.path.basename(f),
                "excerpt": text
            })
            
            print(f"üìä PDF {i+1}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # Small delay to show progress
            await asyncio.sleep(0.2)
        
        await self.send_update("local_documents", "active", 55, f"–¢–µ–∫—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        prompt = self.get_local_documents_prompt(files_payload, research_data, research_type)
        await self.send_update("local_documents", "active", 65, "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        try:
            await self.send_update("local_documents", "active", 70, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –ò–ò...")
            content = await self._call_deepseek(prompt, temperature=0.2, max_new_tokens=1024)
            await self.send_update("local_documents", "active", 85, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –ò–ò...")
        except Exception as e:
            await self.send_update("local_documents", "error", 0, f"API Error: {e}")
            return {"insights": [], "files": [f["file"] for f in files_payload]}
        
        await self.send_update("local_documents", "active", 90, "–ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã...")
        insights = self.parse_local_insights(content)
        
        # Count insights by source file
        insights_by_file = {}
        for insight in insights:
            source_file = insight.get("source_file", "unknown.pdf")
            if source_file not in insights_by_file:
                insights_by_file[source_file] = 0
            insights_by_file[source_file] += 1
        
        # Create summary message without specific file names
        summary = f"–ù–∞–π–¥–µ–Ω–æ {len(insights)} –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–∑ {len(files_payload)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
        
        await self.send_update("local_documents", "completed", 100, summary)
        
        print(f"üìà –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò PDF:")
        print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(files_payload)}")
        print(f"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–Ω—Å–∞–π—Ç–æ–≤: {len(insights)}")
        for file, count in insights_by_file.items():
            print(f"   {file}: {count} –∏–Ω—Å–∞–π—Ç–æ–≤")
        
        return {"insights": insights, "files": [f["file"] for f in files_payload]}
    
    async def analyze_cases(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Stage 2: Analyze cases with retry mechanism"""
        return await self._execute_with_retry(
            self._analyze_cases_internal,
            market_data,
            research_data,
            research_type,
            stage_name="case_analysis",
            stage_description="–∞–Ω–∞–ª–∏–∑–∞ –∫–µ–π—Å–æ–≤"
        )
    
    async def _analyze_cases_internal(self, market_data: Dict[str, Any], research_data: Dict[str, Any], research_type: str) -> List[Dict[str, Any]]:
        """Internal method for case analysis"""
        await self.send_update("case_analysis", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –∫–µ–π—Å–æ–≤...")
        
        prompt = self.get_case_analysis_prompt(market_data, research_data, research_type)
        
        await self.send_update("case_analysis", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑...")
        
        try:
            content = await self._call_deepseek(prompt, temperature=0.5, max_new_tokens=2048)
            await self.send_update("case_analysis", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞...")

            await self.send_update("case_analysis", "active", 90, "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–µ–º –∫–µ–π—Å—ã...")

            cases = self.parse_cases(content)
            await self.send_update("case_analysis", "completed", 100, f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(cases)} –∫–µ–π—Å–æ–≤")

            return cases
        except Exception as e:
            raise Exception(f"API Error: {e}")
    
    
    
    
    async def generate_report(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Stage 4: Generate final report with retry mechanism"""
        return await self._execute_with_retry(
            self._generate_report_internal,
            cases,
            research_data,
            research_type,
            stage_name="report_generation",
            stage_description="–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
        )
    
    async def _generate_report_internal(self, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Internal method for report generation"""
        await self.send_update("report_generation", "active", 10, "–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—á–µ—Ç–∞...")
        
        prompt = self.get_report_generation_prompt(cases, research_data, research_type)
        
        await self.send_update("report_generation", "active", 30, "–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –æ—Ç—á–µ—Ç–∞...")
        
        try:
            response_text = await self._call_deepseek(prompt, temperature=0.3, max_new_tokens=4096)

            await self.send_update("report_generation", "active", 70, "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç...")
            await self.send_update("report_generation", "active", 90, "–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç...")

            report_content = self.extract_report_content(response_text)

            # Enhance report with additional links
            await self.send_update("report_generation", "active", 95, "–î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏...")
            enhanced_report = await self.enhance_report_with_links(report_content, cases, research_data, research_type)

            # Clean the report content before final processing
            final_report = self.clean_report_content(enhanced_report)

            # Final report length check
            print(f"üìä –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
            print(f"   –î–ª–∏–Ω–∞ –æ—Ç—á–µ—Ç–∞: {len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤: {final_report.count(chr(10)) + 1}")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫: {final_report.count('[')}")

            await self.send_update("report_generation", "completed", 100, f"–û—Ç—á–µ—Ç –≥–æ—Ç–æ–≤! ({len(final_report)} —Å–∏–º–≤–æ–ª–æ–≤)")

            return final_report
        except Exception as e:
            raise Exception(f"API Error: {e}")
    
    def get_data_collection_prompt(self, research_data: Dict[str, Any], research_type: str) -> str:
        """Get prompt for data collection stage"""
        if research_type == "feature":
            return f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–æ–∏—Å–∫—É –∏ —Å–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö –æ —Ñ–∏–Ω—Ç–µ—Ö-–ø—Ä–æ–¥—É–∫—Ç–∞—Ö.

–¶–ï–õ–¨: –ù–∞–π—Ç–∏ –∏ —Å–æ–±—Ä–∞—Ç—å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–û–î–†–û–ë–ù–£–Æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ñ–∏—á—É "{research_data.get('research_element', '')}".

–ü–ê–†–ê–ú–ï–¢–†–´ –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø:
- –ü—Ä–æ–¥—É–∫—Ç: {research_data.get('product_description', '')}
- –°–µ–≥–º–µ–Ω—Ç: {research_data.get('segment', '')}
- –≠–ª–µ–º–µ–Ω—Ç: {research_data.get('research_element', '')}
- –ë–µ–Ω—á–º–∞—Ä–∫–∏: {research_data.get('benchmarks', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∏–≥—Ä–æ–∫–∏: {research_data.get('required_players', '')}
- –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã: {research_data.get('required_countries', '')}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ü–û–ò–°–ö –°–°–´–õ–û–ö:
1. –ù–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 15-20 –∫–æ–º–ø–∞–Ω–∏–π
2. –î–ª—è –ö–ê–ñ–î–û–ô –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞–π–¥–∏ –ú–ò–ù–ò–ú–£–ú 8-10 –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –°–°–´–õ–û–ö:
   - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –∫–æ–º–ø–∞–Ω–∏–∏
   - –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ (LinkedIn, Twitter, Facebook)
   - –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏
   - –ö–µ–π—Å—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–∑—ã–≤—ã
   - –ü—Ä–µ—Å—Å-—Ä–µ–ª–∏–∑—ã –∏ –Ω–æ–≤–æ—Å—Ç–∏
@@ -1071,135 +936,115 @@ class ResearchProcessor:
                        "fact": item.get("fact") or "",
                        "metrics": item.get("metrics") or None,
                        "date": item.get("date") or None,
                        "links": item.get("links") or []
                    })
                return norm
        except Exception:
            pass
        # Fallback: extract lines starting with '-' or '*'
        insights: List[Dict[str, Any]] = []
        for line in content_str.split('\n'):
            line = line.strip(" -‚Ä¢*")
            if not line:
                continue
            insights.append({
                "source_file": "unknown.pdf",
                "download_link": None,  # Don't create link for unknown files
                "section": "",
                "fact": line,
                "metrics": None,
                "date": None,
                "links": []
            })
        return insights
    
    def parse_market_data(self, content: str, research_type: str) -> Dict[str, Any]:
        """Parse market data from generated text"""
        try:
            companies = self.extract_companies_from_text(content)

            return {
                "raw_content": content,
                "companies": companies,
                "research_type": research_type,
                "timestamp": datetime.now().isoformat(),
                "total_found": len(companies)
            }
        except Exception as e:
            return {
                "raw_content": f"Error parsing data: {str(e)}",
                "companies": [],
                "research_type": research_type,
                "timestamp": datetime.now().isoformat(),
                "total_found": 0
            }
    
    def extract_companies_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract company/product information from text"""
        companies = []
        lines = text.split('\n')
        
        current_company = {}
        for line in lines:
            line = line.strip()
            if not line:
                if current_company:
                    companies.append(current_company)
                    current_company = {}
                continue
                
            # Look for company/product patterns - support both
            if any(keyword in line.lower() for keyword in ['–∫–æ–º–ø–∞–Ω–∏—è:', 'company:', '–Ω–∞–∑–≤–∞–Ω–∏–µ:', 'name:', '–ø—Ä–æ–¥—É–∫—Ç:', 'product:']):
                if current_company:
                    companies.append(current_company)
                current_company = {"name": line.split(':', 1)[1].strip() if ':' in line else line}
            elif any(keyword in line.lower() for keyword in ['—Å–∞–π—Ç:', 'website:', 'url:']):
                if current_company:
                    current_company["website"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif any(keyword in line.lower() for keyword in ['—Å—Ç—Ä–∞–Ω–∞:', 'country:']):
                if current_company:
                    current_company["country"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif any(keyword in line.lower() for keyword in ['—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:', 'characteristics:']):
                if current_company:
                    current_company["characteristics"] = line.split(':', 1)[1].strip() if ':' in line else line
            elif line.startswith('http'):
                if current_company:
                    if "links" not in current_company:
                        current_company["links"] = []
                    current_company["links"].append(line)
        
        if current_company:
            companies.append(current_company)
            
        return companies
    
    def parse_cases(self, content: str) -> List[Dict[str, Any]]:
        """Parse cases from generated text"""
        try:
            return self.extract_cases_from_text(content)
        except Exception:
            return []
    
    def extract_cases_from_text(self, text: str) -> List[Dict[str, Any]]:
        """Extract case information from text"""
        cases = []
        lines = text.split('\n')
        
        current_case = {}
        case_number = 1
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Look for case patterns - support both feature and product cases
            if (line.startswith(f'**–ö–µ–π—Å {case_number}') or 
                line.startswith(f'–ö–µ–π—Å {case_number}') or
                line.startswith(f'**–ü—Ä–æ–¥—É–∫—Ç {case_number}') or
                line.startswith(f'–ü—Ä–æ–¥—É–∫—Ç {case_number}')):
                if current_case:
                    cases.append(current_case)
                current_case = {
                    "number": case_number,
                    "title": line.replace('**', '').replace('*', '').strip()
@@ -1240,167 +1085,115 @@ class ResearchProcessor:
        broken_links = 0
        
        for case in cases:
            if "verified_links" in case:
                total_links += len(case["verified_links"])
                working_links += len([link for link in case["verified_links"] if link.get("status") == "working"])
            if "broken_links" in case:
                broken_links += len(case["broken_links"])
        
        percentage = (working_links/total_links*100) if total_links > 0 else 0
        
        verification_summary = f"""

## –°–≤–æ–¥–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Å—ã–ª–æ–∫

- **–í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ:** {total_links}
- **–†–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {working_links}
- **–ù–µ—Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {broken_links}
- **–ü—Ä–æ—Ü–µ–Ω—Ç —Ä–∞–±–æ—á–∏—Ö —Å—Å—ã–ª–æ–∫:** {percentage:.1f}%

*–í—Å–µ —Å—Å—ã–ª–∫–∏ –±—ã–ª–∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å.*
"""
        
        return report_content + verification_summary
    
    def extract_report_content(self, response_text: str) -> str:
        """Extract report content from generated text"""
        return response_text or "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞"
    
    async def enhance_report_with_links(self, report_content: str, cases: List[Dict[str, Any]], research_data: Dict[str, Any], research_type: str) -> str:
        """Enhance report with additional links from verified sources"""
        try:
            # Extract all verified links from cases
            all_verified_links = []
            for case in cases:
                if "verified_links" in case:
                    for link in case["verified_links"]:
                        if link.get("status") == "working":
                            all_verified_links.append({
                                "url": link.get("url"),
                                "company": case.get("title", case.get("company", "Unknown")),
                                "context": case.get("description", "")
                            })
            
            if not all_verified_links:
                return report_content
            
            # Create prompt for link enhancement
            # Use full report content, but limit to reasonable size for API
            max_content_length = 15000  # Increased from 3000
            report_preview = report_content[:max_content_length]
            if len(report_content) > max_content_length:
                report_preview += "\n\n[... –æ—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –æ—Ç—á–µ—Ç–∞ ...]"
            
            prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç—ã. –£–ª—É—á—à–∏ –æ—Ç—á–µ—Ç, –¥–æ–±–∞–≤–∏–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–û–¢–ß–ï–¢ –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø:
{report_preview}

–ü–†–û–í–ï–†–ï–ù–ù–´–ï –°–°–´–õ–ö–ò:
{json.dumps(all_verified_links[:20], ensure_ascii=False, indent=2)}

–ó–ê–î–ê–ß–ê:
1. –ù–∞–π–¥–∏ –≤ –æ—Ç—á–µ—Ç–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–∫—Ç–æ–≤
2. –î–æ–±–∞–≤—å –∫ –Ω–∏–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: [—Ç–µ–∫—Å—Ç](—Å—Å—ã–ª–∫–∞)
4. –ù–ï –∏–∑–º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç—á–µ—Ç–∞, —Ç–æ–ª—å–∫–æ –¥–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏
5. –ú–∞–∫—Å–∏–º—É–º 3-5 —Å—Å—ã–ª–æ–∫ –Ω–∞ –∞–±–∑–∞—Ü
6. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∞–π—Ç—ã > –∫–µ–π—Å—ã > –Ω–æ–≤–æ—Å—Ç–∏
7. –í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –ü–û–õ–ù–´–ô –æ—Ç—á–µ—Ç —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –Ω–µ –æ–±—Ä–µ–∑–∞–π –µ–≥–æ

–í–ï–†–ù–ò –ü–û–õ–ù–´–ô –£–õ–£–ß–®–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –° –î–û–ë–ê–í–õ–ï–ù–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò.
"""
            
            try:
                enhanced_content = await self._call_deepseek(prompt, temperature=0.3, max_new_tokens=4096)

                if enhanced_content:
                    print(f"üìä –£–õ–£–ß–®–ï–ù–ò–ï –û–¢–ß–ï–¢–ê:")
                    print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {len(report_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    print(f"   –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {len(enhanced_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return enhanced_content
                else:
                    print(f"‚ö†Ô∏è –ò–ò –Ω–µ –≤–µ—Ä–Ω—É–ª —É–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π")
                    return report_content
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
                return report_content

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
            return report_content
    
    async def verify_report_links(self, report_content: str) -> str:
        """Verify all links in the report and remove broken ones"""
        try:
            import re
            
            # Find all markdown links in the report
            link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
            links = re.findall(link_pattern, report_content)
            
            if not links:
                print("üìã –°—Å—ã–ª–∫–∏ –≤ –æ—Ç—á–µ—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return report_content
            
            print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            verified_links = []
            broken_links = []
            
            # Check each link
            for i, (text, url) in enumerate(links):
                print(f"üîó –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Å—ã–ª–∫—É {i+1}/{len(links)}: {url}")t
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å—Å—ã–ª–æ–∫ –≤ –æ—Ç—á–µ—Ç–µ: {str(e)}")
            return report_content
    
